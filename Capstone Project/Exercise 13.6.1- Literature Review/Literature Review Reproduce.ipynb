{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4445534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "import pydicom as dcm\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3f73a6-470f-4d39-9d0e-5b19781d82f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f778cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 07:01:52.860527: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-20 07:01:53.430911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38444 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "inception = keras.applications.resnet.ResNet101(include_top=False, input_shape=(512,512,3), weights='imagenet', \n",
    "                                                       classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bf27e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inception.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58173b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_and_width(dicom):\n",
    "    return tuple([int(x[0]) if type(x) == dcm.multival.MultiValue else int(x) for x in [dicom.WindowCenter, dicom.WindowWidth]])\n",
    "def normalize_minmax(img):\n",
    "    mi, ma = img.min(), img.max()\n",
    "    return (img - mi) / (ma - mi)\n",
    "\n",
    "def window_filter(img, center, width, slope, intercept):\n",
    "    out = np.copy(img)\n",
    "    out = out*slope + intercept\n",
    "    lowest_visible = center - width//2\n",
    "    highest_visible = center + width//2\n",
    "    \n",
    "    out[out < lowest_visible] = lowest_visible\n",
    "    out[out > highest_visible] = highest_visible\n",
    "    return normalize_minmax(out)\n",
    "def standardize(img):\n",
    "    m, std = img.mean(), img.std()\n",
    "    return (img - m) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a0339a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(inception)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef2e8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet101 (Functional)      (None, 16, 16, 2048)      42658176  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 12294     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,670,470\n",
      "Trainable params: 42,565,126\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d280664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=['binary_accuracy'],\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath='reproduce_training/checkpoint.ckpt',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6024ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tens = tf.convert_to_tensor(data)\n",
    "# model.predict(data_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6251fd22-b047-4484-8edb-bf9069c12e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_img_tensor(img_path):\n",
    "#     dicom = dcm.dcmread(img_path, force=True)\n",
    "    \n",
    "#     img = dicom.pixel_array\n",
    "#     center, width = get_center_and_width(dicom)\n",
    "#     slope, intercept = dicom.RescaleSlope, dicom.RescaleIntercept\n",
    "#     brain = window_filter(img, 40, 80, slope, intercept)\n",
    "#     subdural = window_filter(img, 80, 200, slope, intercept)\n",
    "#     tissue = window_filter(img, 40, 380, slope, intercept)\n",
    "    \n",
    "#     return tf.convert_to_tensor(np.stack([brain, subdural, tissue], axis=2), dtype=tf.dtypes.float32)\n",
    "def get_img_tensor(img_path):\n",
    "    return tf.convert_to_tensor(np.asarray(Image.open(img_path), dtype=np.float32) / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01bfbb9a-9098-4275-910a-38d186a02854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNASequence(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = [self.y[img_id.split('/')[-1].split('.')[0]] for img_id in batch_x]\n",
    "        # batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        return (tf.stack([get_img_tensor(img_path) for img_path in batch_x], axis=0), \n",
    "               tf.convert_to_tensor(batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25f9501c-3d6f-4df0-aae6-394eef3b2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_path = 'rsna-intracranial-hemorrhage-detection/stage_2_train_imgs/'\n",
    "labels = pd.read_csv('rsna-intracranial-hemorrhage-detection/train_labels.csv')\n",
    "labels = {l[0]: l[1:].astype(np.int8) for l in labels.to_numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02559227-3636-471b-a08a-bb57621ba716",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cutoff = 12800 #training the whole dataset takes ~9 hours, so we cut it short for proof-of-concept purposes.\n",
    "train_sequence = RSNASequence([png_path + img_name for img_name in os.listdir(png_path)[:train_cutoff]], labels, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be0bc250-b549-4f70-8b9a-07c80831ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 07:02:13.111712: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "2021-12-20 07:02:16.036751: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - ETA: 0s - loss: 0.1352 - binary_accuracy: 0.9533\n",
      "Epoch 00001: saving model to reproduce_training/checkpoint.ckpt\n",
      "400/400 [==============================] - 561s 1s/step - loss: 0.1352 - binary_accuracy: 0.9533\n",
      "Epoch 2/2\n",
      "400/400 [==============================] - ETA: 0s - loss: 0.0966 - binary_accuracy: 0.9643\n",
      "Epoch 00002: saving model to reproduce_training/checkpoint.ckpt\n",
      "400/400 [==============================] - 228s 570ms/step - loss: 0.0966 - binary_accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9637543f50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_sequence, epochs=2, callbacks=[cp_callback])\n",
    "# model.predict(train_sequence[1][0])\n",
    "# train_sequence[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc9b67-a2fd-4324-ba13-a26fe55713d9",
   "metadata": {},
   "source": [
    "We can see that training on just a few thousand of the 700k+ DICOM training images yields a 96% accuracy on the training dataset, using ResNet101 pretrained weights as a base and fine tuning using a custom softmax binary cross-entropy layer. These preliminary results are very promising, however it is unknown whether the model can generalize to the entire training set let alone the test set. However, I believe that I have succeeded in reproducing one part of the research that I have conducted- all solutions I found used a CNN with pretrained weights as the first stage for their model. While reproducing the entirety of their research would take up to a week of coding, debugging, and training (not to mention money); I believe that my results are sufficient for this capstone submission (which predicted a 5-20 hour timeframe). \n",
    "\n",
    "When testing full models, I plan to start by training at least 1 epoch on the full dataset with the CNN, then expirement with feeding the results and the extracted features (taken from the CNN's pennultimate layer) to feed into:\n",
    "1. An ensemble method of boosting or bagging tree methods (as one of my sources did)\n",
    "2. A RNN, perhaps a LSTM\n",
    "3. Another CNN\n",
    "4. An ensemble of RNNs and/or CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db075f-5109-4d8d-915c-5e7a3092dd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
