{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aedc4ab2-54e2-4a13-8919-e5239dad5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dcm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import csv\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d60781-b8f4-4912-bda4-5741b2121ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "train_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_train_imgs/'\n",
    "train_label_path = 'rsna-intracranial-hemorrhage-detection/train_labels.csv'\n",
    "train_ct_path = 'rsna-intracranial-hemorrhage-detection/train_ct_scans.csv'\n",
    "train_coord_path = 'rsna-intracranial-hemorrhage-detection/train_ct_coords.csv'\n",
    "base_model_path = '/home/jupyter/base-cnn-model/checkpoint.ckpt'\n",
    "\n",
    "saved_model_path = 'experiment-3-checkpoints/checkpoint.ckpt'\n",
    "\n",
    "test_img_dir = 'rsna-intracranial-hemorrhage-detection/stage_2_test_imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48720c46-bb9c-4c65-b7f6-7d1bfd567787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_tensor(img_path):\n",
    "    return tf.convert_to_tensor(np.asarray(Image.open(img_path), dtype=np.float32) / 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9922a30b-d364-4d8b-9a05-a88a5b49bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNASequence(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A keras Sequence which provides training data to the model\n",
    "    \"\"\"\n",
    "    def __init__(self, labels, train_cutoff, batch_size, extractor, img_dir, n_slices, \\\n",
    "                train_img_ct, train_img_ct_ind, feature_dir='extracted_features/', ):\n",
    "        self.x = None\n",
    "        self.y = labels #DataFrame of all labels for the whole training set\n",
    "        self.train_cutoff = train_cutoff #number of images to predict for in one epoch\n",
    "        self.batch_size = batch_size #number of image clusters to train on in one batch\n",
    "        self.img_dir = img_dir #directory containing PNG images for training\n",
    "        self.n_slices = n_slices #number of slices both above and below, to collect in a single training data point\n",
    "        self.extractor = extractor #extractor model\n",
    "        self.feature_dir = feature_dir #directory containing .npy files which are feature vectors of each image. The directory may be incomplete;\n",
    "                                        #if so, this class will automatically generate feature vectors as needed\n",
    "        self.train_img_ct = train_img_ct\n",
    "        self.train_img_ct_ind = train_img_ct_ind\n",
    "        \n",
    "        self.on_epoch_end() #initialize a random subset of data\n",
    "    \n",
    "    def get_nearby_slices_names(self, img_id, n_slices):\n",
    "        \"\"\"\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 IDs of nearby images\n",
    "        \n",
    "        Gets a list of names of the nearby image slices, rather than their feature vectors or raw image values\n",
    "        \"\"\"\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "        return ct[low:high+1]\n",
    "    \n",
    "    def get_nearby_slices(self, img_id, n_slices):\n",
    "        '''\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 images as TensorFlow tensor\n",
    "        \n",
    "        Will retrieve n_slices slices from BOTH above and below the given image. If there is not enough space, it will\n",
    "        add more slices either below (if the image is near the top of the scan) or above (if the image is near the bottom of the scan).\n",
    "        Exactly 2*n_slices + 1 images will be returned.\n",
    "        '''\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "        return [get_img_tensor(self.train_img_dir+img_id+'.png') for img_id in ct[low:high+1]]\n",
    "\n",
    "    def get_nearby_slices_features(self, img_id, n_slices):\n",
    "        \"\"\"\n",
    "        :img_id (str): the ID of the slice to find nearby slices for\n",
    "        :n_slices (int): number of slices, both above and below, to return.\n",
    "        \n",
    "        :return: list of 2*n_slices + 1 feature vectors arranged in a 2D Tensor\n",
    "        \n",
    "        Gets the feature vectors of nearby slices rather than their raw images. Results in better performance.\n",
    "        \"\"\"\n",
    "        ct_ind, ind = self.train_img_ct_ind[img_id]['ct_ind'], self.train_img_ct_ind[img_id]['ind']\n",
    "        ct = self.train_img_ct[ct_ind]\n",
    "        n = len(ct)\n",
    "        low, high = ind-n_slices, ind+n_slices\n",
    "        if low < 0:\n",
    "            high += abs(low)\n",
    "            low = 0\n",
    "        elif high >= n:\n",
    "            low -= high - (n-1)\n",
    "            high = n-1\n",
    "\n",
    "        res = []\n",
    "        for img_id in ct[low:high+1]:\n",
    "            try:\n",
    "                res.append(np.load(self.feature_dir+img_id+'.npy'))\n",
    "            except:\n",
    "                pass\n",
    "        return tf.squeeze(res)\n",
    "        \n",
    "    def precompute_features(self):\n",
    "        \"\"\"\n",
    "        Passes all the images through the extractor model first before training and save them in self.feature_dir as \n",
    "        feature vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        to_compute = set()\n",
    "        present = set(x.split('.')[0] for x in os.listdir(self.feature_dir))\n",
    "        print('Collecting necessary slices...')\n",
    "        for img_id in tqdm(self.x):\n",
    "            ct = self.train_img_ct[self.train_img_ct_ind[img_id]['ct_ind']]\n",
    "            to_compute.update(ct)\n",
    "            \n",
    "        print(f'{len(to_compute.intersection(present))} feature vectors already present')\n",
    "        to_compute = list(to_compute.difference(present))\n",
    "        print(f'Computing {len(to_compute)} new feature vectors...')\n",
    "        compute_batch_size = 200\n",
    "        \n",
    "        if len(to_compute) == 0:\n",
    "            return\n",
    "        \n",
    "        for i in tqdm(range(ceil(len(to_compute)//compute_batch_size)+1)):\n",
    "            batch_names = to_compute[i*compute_batch_size : (i+1)*compute_batch_size]\n",
    "            batch = np.array(Parallel(n_jobs=-1, backend='threading')(delayed(get_img_tensor)(self.img_dir+img_id+'.png') for img_id in batch_names))\n",
    "            try:\n",
    "                batch = self.extractor.predict(batch)\n",
    "            except:\n",
    "                print(\"ERROR\")\n",
    "                print(batch.shape)\n",
    "                sys.exit(0)\n",
    "            for i,feat_vec in enumerate(batch):\n",
    "                np.save(self.feature_dir+batch_names[i]+'.npy', feat_vec)\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_x_names = [self.get_nearby_slices_names(img_id, self.n_slices) for img_id in batch_x]\n",
    "        batch_y = tf.convert_to_tensor([np.array([self.y[img_id] for img_id in slices]).flatten() for slices in batch_x_names])\n",
    "        batch_x = tf.convert_to_tensor([[np.squeeze(np.load(self.feature_dir+img_id+'.npy')) for img_id in slices] for slices in batch_x_names])\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        ind = np.random.choice(list(range(len(os.listdir(self.img_dir)))), size=self.train_cutoff, replace=False)\n",
    "        self.x = [img_name.split('.')[0] for img_name in np.array(os.listdir(self.img_dir))[ind]]\n",
    "        self.precompute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ee137dc-06c0-492e-809f-d7270d2ea066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNAModel:\n",
    "    def __init__(self, train_img_dir, train_label_path, train_ct_path, train_coord_path, base_model_path, feature_dir):\n",
    "        self.train_img_dir = train_img_dir #directory containing all 3-channel PNG images of scans\n",
    "        self.train_label_path = train_label_path #path to CSV file containing binary encodings of the labels\n",
    "        self.train_ct_path = train_ct_path #path to CSV where each line lists all image IDs in a certain CT scan\n",
    "        self.train_coord_path = train_coord_path #path to CSV where each line contains an image ID and its (x,y,z) coordinates in its CT \n",
    "        self.base_model_path = base_model_path #path to the base CNN model which will be used to extract features \n",
    "        self.feature_dir = feature_dir #directory containing .npy files which are feature vectors of each image. The directory may be incomplete;\n",
    "                                        #if so, this model will automatically generate feature vectors as needed\n",
    "    \n",
    "        self.train_img_ct = None\n",
    "        self.train_img_ct_ind = None\n",
    "        self.labels = None\n",
    "        self.extractor = None\n",
    "        self.model = None\n",
    "        self.callbacks = None\n",
    "    \n",
    "        self.assemble_ct_scans()\n",
    "        self.retrieve_labels()\n",
    "        self.retrieve_feature_extractor()\n",
    "        \n",
    "    def assemble_ct_scans(self):\n",
    "        \"\"\"\n",
    "        Collects image IDs from the same CT scan and stores them in a dictionary in sorted order (increasing z-value)\n",
    "        \"\"\"\n",
    "        self.train_img_ct = {} # scan index : list of image IDs in the scan\n",
    "        self.train_img_ct_ind = {} #image ID : {\"ct_ind\": index of the CT scan this image belongs to (key in train_img_ct), \"ind\": index in the list of slices}\n",
    "        i = 0\n",
    "        \n",
    "        train_img_coords = pd.read_csv(self.train_coord_path, index_col=0, names=['x','y','z'])\n",
    "        def populate_ct_info(row,i):\n",
    "            #takes in a list of Image IDs of slices in a CT scan\n",
    "            row = row[1:]\n",
    "            row.sort(key=lambda x: train_img_coords.loc[x]['z'])\n",
    "            for slice_ind, img_id in enumerate(row):\n",
    "                self.train_img_ct_ind[img_id] = {'ct_ind': i, 'ind': slice_ind}\n",
    "            self.train_img_ct[i] = row\n",
    "\n",
    "        \n",
    "        with open(self.train_ct_path) as scans:\n",
    "            reader = csv.reader(scans, delimiter=',')\n",
    "            Parallel(n_jobs=-1, backend='threading', require='sharedmem', batch_size=75)(delayed(populate_ct_info)(row,i) for i, row in tqdm(list(enumerate(list(reader)))))\n",
    "            \n",
    "    def retrieve_labels(self):\n",
    "        \"\"\"\n",
    "        Retrieves labels from the train_label_path and stores them in a DataFrame\n",
    "        \"\"\"\n",
    "        labels = pd.read_csv(self.train_label_path)\n",
    "        self.labels = {l[0]: l[1:].astype(np.int8) for l in labels.to_numpy()}\n",
    "        \n",
    "    def retrieve_feature_extractor(self):\n",
    "        \"\"\"\n",
    "        Retrieves the model to be used for feature extraction\n",
    "        \"\"\"\n",
    "        base_model = keras.models.load_model(self.base_model_path)\n",
    "        self.extractor = keras.models.Sequential(base_model.layers[:-1])\n",
    "    \n",
    "    def initialize_model(self, saved_model_path=None):\n",
    "        \"\"\"\n",
    "        :saved_model_path (str): The directory containing files for the saved model, including 'saved_model.pb'. If None,\n",
    "        this method will create a new model.\n",
    "        \n",
    "        Initialize the RNN model for training, either by creating a new one or using a previously\n",
    "        saved model path.\n",
    "        \"\"\"\n",
    "        if not saved_model_path: \n",
    "            #create a new model if no checkpoint was provided\n",
    "            self.model = keras.Sequential([Bidirectional(LSTM(512, return_sequences=True, name='lstm0')),\n",
    "                              Bidirectional(LSTM(512, return_sequences=True, name='lstm1')),\n",
    "                              Bidirectional(GRU(256, return_sequences=True, name='gru0')),\n",
    "                              Conv1D(6, 1, padding='same', activation='sigmoid'),\n",
    "                              Flatten()\n",
    "                             ])\n",
    "            print('Created new model')\n",
    "        else:\n",
    "            self.model = keras.models.load_model(saved_model_path)\n",
    "            print(f'Found model at {saved_model_path}')\n",
    "        \n",
    "        self.model.build(input_shape=(None, 19,2048))\n",
    "        self.model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
    "              metrics=['binary_accuracy', \n",
    "                       keras.metrics.AUC(multi_label=True, num_labels=114, from_logits=False),\n",
    "                       keras.metrics.Precision(), keras.metrics.Recall()],\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=3e-5))\n",
    "        \n",
    "    \n",
    "    def train(self, batch_size=32, n_batches=2000, n_slices=9, n_epochs=2, save_path=None):\n",
    "        if not self.model:\n",
    "            print(\"ERROR: please call self.initialize_model first before training\")\n",
    "            return\n",
    "        \n",
    "        train_cutoff = batch_size*n_batches\n",
    "        train_sequence = RSNASequence(self.labels, train_cutoff, batch_size, self.extractor, self.train_img_dir, n_slices, \\\n",
    "                                     self.train_img_ct, self.train_img_ct_ind, self.feature_dir)\n",
    "        if save_path:\n",
    "            self.callbacks = [keras.callbacks.ModelCheckpoint(filepath=save_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)]\n",
    "        \n",
    "        self.model.fit(x=train_sequence, epochs=n_epochs, callbacks=self.callbacks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86b2b066-b8cc-4e55-8d4d-bc3a8dc6a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18931/18931 [00:46<00:00, 410.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model = RSNAModel(train_img_dir, train_label_path, train_ct_path, train_coord_path, base_model_path, feature_dir='extracted_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aeae402-f9ac-4f27-87de-2928a9216f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model at experiment-3-checkpoints/checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "model.initialize_model(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "437afcf4-9e96-4e89-8a53-4cd399aceb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting necessary slices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64000/64000 [00:00<00:00, 233096.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722933 feature vectors already present\n",
      "Computing 0 new feature vectors...\n",
      "Epoch 1/2\n",
      "  83/2000 [>.............................] - ETA: 21:00 - loss: 0.0413 - binary_accuracy: 0.9849 - auc_1: 0.9859 - precision_1: 0.9041 - recall_1: 0.8552"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3321/2135423642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3321/2105093058.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, n_batches, n_slices, n_epochs, save_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m                                                  verbose=1)]\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(save_path=saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9fc21-9d9d-405c-b15d-5baa70db9c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
