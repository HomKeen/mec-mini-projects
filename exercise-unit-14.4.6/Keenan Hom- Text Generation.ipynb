{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad55a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbdb90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n",
      "1130496/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715da2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b56302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d514c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e143203f",
   "metadata": {},
   "source": [
    "# Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a774ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 06:21:30.243602: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-17 06:21:32.021028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38444 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df76231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193e70c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea106fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "785ff582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9838c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f676625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4025fadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65afedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "506138a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6def12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6458097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79758d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be8ff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c96d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cf04263",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e787bb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8bb706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ec3d1",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7596c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b0d3b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94a641d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd266cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 06:21:36.704951: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 06:21:38.953052: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00550f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be4728c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([56, 29, 27, 51, 64, 41, 14, 64, 39, 42,  7, 32, 48, 48, 58, 20,  9,\n",
       "        4, 62, 26, 31, 14, 65, 33, 22, 33,  7, 32, 27, 26, 15,  2, 60, 49,\n",
       "       63, 14,  8, 58, 57, 29, 47, 58,  1, 48, 57, 15, 17, 51, 53, 49, 54,\n",
       "       24, 17, 33, 65,  7, 38, 33, 60, 43, 35, 13, 41, 24, 63, 55,  4,  7,\n",
       "       33, 20, 24, 51, 58, 63, 48, 44, 48, 54,  9, 39, 44, 12, 57, 47, 11,\n",
       "        7, 53, 41,  2, 64, 22,  3, 30, 41, 28, 49, 26, 39, 17, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bc6fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'sed;\\nAnd I have thrust myself into this maze,\\nHaply to wive and thrive as best I may:\\nCrowns in my p'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'qPNlybAyZc,SiisG.$wMRAzTIT,SNMB ujxA-srPhs\\nirBDlnjoKDTz,YTudV?bKxp$,TGKlsxieio.Ze;rh:,nb yI!QbOjMZD3'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5d92547",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9279c312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.191521\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15924b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.1233"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bacadacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97623181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdf5ac",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3b0d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 5s 12ms/step - loss: 2.6996\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.9715\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.6959\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.5411\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.4451\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.3787\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.3285\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.2836\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.2433\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.2046\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.1649\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.1249\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.0824\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 1.0366\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.9897\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.9388\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.8877\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.8360\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.7849\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 3s 12ms/step - loss: 0.7364\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae5a1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1df375b1-f8ce-45ba-942b-926fd4d84050",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4ba2e3a-b890-4e5f-abf8-512f316a8421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Of man, the murderous warding love?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You worthy Montague, content thee thither\n",
      "Were best mest comfort that are flinty thousand men\n",
      "More to be shorten'd in your own turgle:\n",
      "Conceive an old tale because your loving marriage,\n",
      "Upon supposed errished fill'd my keeping feel;\n",
      "And, there, talking to yourself,\n",
      "Your lady dress'd his covery: canst supter\n",
      "Bectred in hell, and makes the royalt fair.\n",
      "Come, Tumblo, when fools where suns there was a truth\n",
      "To break his neck and bears; the\n",
      "Muke was a pack-horse in a noble service.\n",
      "\n",
      "TRANIO:\n",
      "As if that royal eye? Tybalt moand, that I know.\n",
      "\n",
      "DORCAS:\n",
      "If that humble suspected this misdreasy\n",
      "Of all their frowand, in the noble inattle\n",
      "Now, our traitor, and that thou beg't more time\n",
      "Than a nicer. Whilst thou liest.\n",
      "\n",
      "GREMIO:\n",
      "Ay.\n",
      "\n",
      "ANTONIO:\n",
      "Which Had he appeareth of the jewelts\n",
      "That stumbled that take from Lizy:\n",
      "Dark else, if you be recourted\n",
      "That the place by holy man promight\n",
      "I dare not soon for 't.\n",
      "\n",
      "PETRUCHIO:\n",
      "Now, bring him truth.\n",
      "\n",
      "PAULINA:\n",
      "Wh \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.3357837200164795\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1026f26c-f757-4837-abed-547c2b356c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe tempted tempest on a petugn\\nof prosence, which is am innocent soul:\\nMy life should be thus to extember\\nThe more my kingdom from the villany that\\nlaw-sworn brother, his folly jest agree,\\nBut, like an unstain'd bulk, such a gariert-neighbour,\\nHow long land dark despair and not an action can before.\\n\\nBRPTUS:\\nIf I had put your majesty, this must be made,\\nAnd I know not her,--O bridenes\\nTo let me seek to think with your love: growning\\nMost dead affording yours, condemn'd my letter;\\nFor 'tis the clouds three hundred that was wont to have\\nA thing that did but helain:\\nWithout repent not to help you,--\\n\\nGREEY:\\n'Tis well betted you again?\\n\\nANGELO:\\nI will tell you well, I meen in bazents\\nHear me but the business, make thee thus\\nmusicians, my tears are but histress while thou liest\\nThat ever was whet his path from the world,\\nSo will my brother, which if you blush,\\nWeeping at the Duke of Catulb by.\\n\\nGRUMIO:\\nAnd yon, it shall be so.\\n\\nCatter:\\nMadam, ha? Cambio Angelo what string passand man!\\nWe \"\n",
      " b\"ROMEO:\\nWhy, thy gods appear to the unking, my former librace,\\nI dared not like a while:\\nPut presently the deputy swelling of your stave: and then read lad,\\nDecart you to Pomp and quietness: my house\\nyet in the sun, or any thing you now?\\n\\nQUEEN MARGARET:\\nAy, Warwick, as I say, if moveables\\nThou waft this special pardon them:\\nYet, for a father's death, why, thou hast undone\\nTo other weak flow; and that his merriments\\nShall bid to seem to pluck him flowers of his descent.\\n\\nADRIAN:\\nThis is a business.\\n\\nLEONTES:\\nO, 'tis at liberty.' and\\nNeither that stays I bear thee in his face.\\nSay, let my heart that thus, our spears through attempt.\\n\\nYORK:\\nSo that is there and Warwick, Petruchio is my queen:\\nAnd when great spectacle of this place I please.\\nWhat Clarence cowards Mariana? Have you no obedience?\\nSaw Romeo, aid you think?\\n\\nQUEEN MARGARET:\\nTo London, to be pratents, cells before to usbagl!\\nHit be, 'Warwiek beasts, my brother Montague!\\nA collam man! how art thou call'st me true?\\n\\nVOLUMNIA:\\nIndeed, I\"\n",
      " b\"ROMEO:\\nThere lies the least senseept, she was yours.\\n\\nKING RICHARD III:\\nKind bring your grace's wife stands thy looks begnant;\\nBut not a joy at a duke but call.\\nHark. I have craving this delight shame\\nTigers us by whip I find that tide.\\n\\nANGELO:\\nWhenk he spranged without more wounded,\\nMore to a most ignorant woman,\\nThat most impute the business presently.\\nBut from denyad, as for as was?\\nArms ere you do profane thee still any grief:\\nTherefore you scand believe this human character,\\nher fulling thunder, with much more changed in extremity.\\nWas the recomform to his unswirtuse sorrow.\\n\\nDORSET:\\nYou must not receive the choice of our abuse.\\n\\nKING EDWARD IV:\\nBrothers on mine own accoutance of the younger.\\nWhereto prince our Hanis mother's face.\\n\\nBENVOLIO:\\nTut, that I fly thus thing fair, my looks.\\n\\nGLOUCESTER:\\nFair well, sir, what last of any mad most uncleander.\\nI thought believe but that another each\\nAnd fly you say I die.\\n\\nTRANIO:\\nWhy should you be rejectious to the rock\\nUnto his burniage, that \"\n",
      " b\"ROMEO:\\nGo you touching one, I will never look pale\\nBefore the consecrated from her whose latest restroy with\\nhim.\\n\\nBoth:\\nWhy then shoils for the\\nsight against this earth you.\\n\\nDUKE VINCENTIO:\\nFrom the cause! 'Cain we speak: for then which pewly, for\\nas desperate winter'd from this rest. But, if you then\\nI will go forthwith.\\n\\nPRINCE EDWARD:\\nNay, to be such a name, a very lion one; and\\n'tis like encloseth my sacred sword;\\nWhich to rejoice his blood pale Norfolk fought:\\nBoats that will find exalts from beautiful ears?\\nPride is to keep you done. Cry when they dash the end of the\\nstar, he is easing. Where is thy word;\\nAnd follow mine honesty: is he\\nremoved the chinfers to my mind, and that with love's work:\\nWhat, note in this daughter?\\n\\nAUDOLYO:\\nThere is a scenant host.\\n\\nShepherd:\\nWhy, would our forcester! Were he is taught before him.\\nSound so he was, how she look'd for nought from gentle:\\nRelocal hurl down, more dreadful musicians,\\nMake not to hide your dukedom he has.\\n\\nMENENIUS:\\nHow welcome hi\"\n",
      " b\"ROMEO:\\nFrom whence, my lord!\\n\\nGLOUCESTER:\\nIs not on him when I was callity shame but his\\nfrailst belong, and I will wrong them hence,\\nAnd peace but weeper, in joy Fronz'd noble,\\nScanst their murderous east.\\n\\nPETRUCHIO:\\nGood mother, I will make at Oxford thing\\nThat if a hair eye or patience.\\n\\nQUEEN MARGARET:\\nO, come, sir; for my loving lord,\\nDo bloss the business I shall follow you.\\n\\nBENVOLIO:\\nGood morrow, quarter, and 'twas the furthest tumble;\\nAnd whether you have but jage-pride them,\\nBut betwixt them with this conquent of the wings.\\nWould he do be spoken with you do;\\nO, let him keep the duke to think of it!\\nThe brother bodies to be most undernate,\\nLong that at nine to live; and therefore I'll give him chastise you:\\nI'll strike, I say; go, gentle king,\\nKall but their life to let him allayius and\\nThe rash might leave to look upon thy bird;\\nFalse to this rescure thrust his bloody days.\\nThe father of that body is my pate,\\nThat I have fought to lay and in their ance which\\nGozng the rock; yet yo\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.2828428745269775\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b94ec-b0b5-47bb-aa0e-a7a5730eb682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
